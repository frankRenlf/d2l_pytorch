{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0834023"
   },
   "source": [
    "# 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）\n",
    "\n",
    "本节我们将在Kaggle上实战狗品种识别问题。\n",
    "本次(**比赛网址是https://www.kaggle.com/c/dog-breed-identification**)。\n",
    " :numref:`fig_kaggle_dog`显示了鉴定比赛网页上的信息。\n",
    "需要一个Kaggle账户才能提交结果。\n",
    "\n",
    "在这场比赛中，我们将识别120类不同品种的狗。\n",
    "这个数据集实际上是著名的ImageNet的数据集子集。与 :numref:`sec_kaggle_cifar10`中CIFAR-10数据集中的图像不同，\n",
    "ImageNet数据集中的图像更高更宽，且尺寸不一。\n",
    "\n",
    "![狗的品种鉴定比赛网站，可以通过单击“数据”选项卡来获得比赛数据集。](../img/kaggle-dog.jpg)\n",
    ":width:`400px`\n",
    ":label:`fig_kaggle_dog`\n"
   ],
   "id": "c0834023"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6107,
     "status": "ok",
     "timestamp": 1698996638015,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     },
     "user_tz": -480
    },
    "id": "z04fvtxhlr_e",
    "outputId": "066817d5-162c-4d38-f9c5-35343ca84e51"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.flush_and_unmount()\n",
    "# drive.mount('/content/drive', force_remount=False)\n"
   ],
   "id": "z04fvtxhlr_e"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Mx8a-EiKlwpz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638015,
     "user_tz": -480,
     "elapsed": 8,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !unzip '/content/drive/MyDrive/Colab Notebooks/dog-breed-identification.zip' -d '/content/drive/MyDrive/data/dog-breed-identification'\n"
   ],
   "id": "Mx8a-EiKlwpz"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1698996638015,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     },
     "user_tz": -480
    },
    "id": "xi6jHPhP159R",
    "outputId": "8c9b1441-119d-4a26-d30c-35a19b4589f0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fri Nov  3 07:30:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0    38W / 300W |   3148MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi"
   ],
   "id": "xi6jHPhP159R"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "5kDIzX2xYMhp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638015,
     "user_tz": -480,
     "elapsed": 5,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !pip install d2l==0.17.6"
   ],
   "id": "5kDIzX2xYMhp"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4d01d08b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638015,
     "user_tz": -480,
     "elapsed": 5,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T11:12:36.674958Z",
     "start_time": "2023-11-03T11:12:34.662096Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "id": "4d01d08b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1698996638015,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     },
     "user_tz": -480
    },
    "id": "mAoVUv4IKspb",
    "outputId": "21b5c817-b739-475a-e36a-7f31e2e76da7",
    "ExecuteTime": {
     "end_time": "2023-11-03T07:33:46.607838Z",
     "start_time": "2023-11-03T07:33:46.475065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/frank/Desktop/d2l-zh/pytorch/chapter_computer-vision\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ],
   "id": "mAoVUv4IKspb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6c7cb76"
   },
   "source": [
    "## 获取和整理数据集\n",
    "\n",
    "比赛数据集分为训练集和测试集，分别包含RGB（彩色）通道的10222张、10357张JPEG图像。\n",
    "在训练数据集中，有120种犬类，如拉布拉多、贵宾、腊肠、萨摩耶、哈士奇、吉娃娃和约克夏等。\n",
    "\n",
    "### 下载数据集\n",
    "\n",
    "登录Kaggle后，可以点击 :numref:`fig_kaggle_dog`中显示的竞争网页上的“数据”选项卡，然后点击“全部下载”按钮下载数据集。在`../data`中解压下载的文件后，将在以下路径中找到整个数据集：\n",
    "\n",
    "* ../data/dog-breed-identification/labels.csv\n",
    "* ../data/dog-breed-identification/sample_submission.csv\n",
    "* ../data/dog-breed-identification/train\n",
    "* ../data/dog-breed-identification/test\n",
    "\n",
    "\n",
    "上述结构与 :numref:`sec_kaggle_cifar10`的CIFAR-10类似，其中文件夹`train/`和`test/`分别包含训练和测试狗图像，`labels.csv`包含训练图像的标签。\n",
    "\n",
    "同样，为了便于入门，[**我们提供完整数据集的小规模样本**]：`train_valid_test_tiny.zip`。\n",
    "如果要在Kaggle比赛中使用完整的数据集，则需要将下面的`demo`变量更改为`False`。\n"
   ],
   "id": "f6c7cb76"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8530da04",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638015,
     "user_tz": -480,
     "elapsed": 3,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T11:12:38.471115Z",
     "start_time": "2023-11-03T11:12:38.444296Z"
    }
   },
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',\n",
    "                            '0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')\n",
    "\n",
    "# 如果使用Kaggle比赛的完整数据集，请将下面的变量更改为False\n",
    "demo = False\n",
    "if demo:\n",
    "    data_dir = d2l.download_extract('dog_tiny')\n",
    "else:\n",
    "    data_dir = os.path.join('..', 'data', 'dog-breed-identification')"
   ],
   "id": "8530da04"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e79f9215"
   },
   "source": [
    "### [**整理数据集**]\n",
    "\n",
    "我们可以像 :numref:`sec_kaggle_cifar10`中所做的那样整理数据集，即从原始训练集中拆分验证集，然后将图像移动到按标签分组的子文件夹中。\n",
    "\n",
    "下面的`reorg_dog_data`函数读取训练数据标签、拆分验证集并整理训练集。\n"
   ],
   "id": "e79f9215"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "97daf7df",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638015,
     "user_tz": -480,
     "elapsed": 3,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T11:12:40.707113Z",
     "start_time": "2023-11-03T11:12:40.697405Z"
    }
   },
   "outputs": [],
   "source": [
    "def reorg_dog_data(data_dir, valid_ratio):\n",
    "    labels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv'))\n",
    "    d2l.reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    d2l.reorg_test(data_dir)\n",
    "\n",
    "\n",
    "batch_size = 32 if demo else 128\n",
    "valid_ratio = 0.1\n",
    "# reorg_dog_data(data_dir, valid_ratio)\n",
    "# len(data_dir), data_dir[0]"
   ],
   "id": "97daf7df"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b665aa35"
   },
   "source": [
    "## [**图像增广**]\n",
    "\n",
    "回想一下，这个狗品种数据集是ImageNet数据集的子集，其图像大于 :numref:`sec_kaggle_cifar10`中CIFAR-10数据集的图像。\n",
    "下面我们看一下如何在相对较大的图像上使用图像增广。\n"
   ],
   "id": "b665aa35"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d109db32",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638015,
     "user_tz": -480,
     "elapsed": 3,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T11:12:42.864777Z",
     "start_time": "2023-11-03T11:12:42.858798Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    # 随机裁剪图像，所得图像为原始面积的0.08～1之间，高宽比在3/4和4/3之间。\n",
    "    # 然后，缩放图像以创建224x224的新图像\n",
    "    torchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
    "                                             ratio=(3.0 / 4.0, 4.0 / 3.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomRotation(degrees=15),\n",
    "    # 随机更改亮度，对比度和饱和度\n",
    "    torchvision.transforms.ColorJitter(brightness=0.4,\n",
    "                                       contrast=0.4,\n",
    "                                       saturation=0.4),\n",
    "    # 添加随机噪声\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # 标准化图像的每个通道\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])"
   ],
   "id": "d109db32"
  },
  {
   "cell_type": "code",
   "source": [
    "# transform_train_swin = torchvision.transforms.Compose([\n",
    "#     # 随机裁剪图像，所得图像为原始面积的0.08～1之间，高宽比在3/4和4/3之间。\n",
    "#     # 然后，缩放图像以创建224x224的新图像\n",
    "#     torchvision.transforms.RandomResizedCrop(256, scale=(0.08, 1.0),\n",
    "#                                              ratio=(3.0 / 4.0, 4.0 / 3.0)),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     torchvision.transforms.RandomRotation(degrees=15),\n",
    "#     # 随机更改亮度，对比度和饱和度\n",
    "#     torchvision.transforms.ColorJitter(brightness=0.4,\n",
    "#                                        contrast=0.4,\n",
    "#                                        saturation=0.4),\n",
    "#     # 添加随机噪声\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     # 标准化图像的每个通道\n",
    "#     torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                                      [0.229, 0.224, 0.225])])"
   ],
   "metadata": {
    "id": "gBAfS2wd43m-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638016,
     "user_tz": -480,
     "elapsed": 4,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    }
   },
   "id": "gBAfS2wd43m-",
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff29a039"
   },
   "source": [
    "测试时，我们只使用确定性的图像预处理操作。\n"
   ],
   "id": "ff29a039"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3bdc1caa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638016,
     "user_tz": -480,
     "elapsed": 4,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T11:12:44.699387Z",
     "start_time": "2023-11-03T11:12:44.693448Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    # 从图像中心裁切224x224大小的图片\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])"
   ],
   "id": "3bdc1caa"
  },
  {
   "cell_type": "code",
   "source": [
    "# transform_test_swin = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize(280),\n",
    "#     # 从图像中心裁切224x224大小的图片\n",
    "#     torchvision.transforms.CenterCrop(256),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                                      [0.229, 0.224, 0.225])])"
   ],
   "metadata": {
    "id": "8sIlHw3r477g",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996638016,
     "user_tz": -480,
     "elapsed": 4,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    }
   },
   "id": "8sIlHw3r477g",
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58f4c8c8"
   },
   "source": [
    "## [**读取数据集**]\n",
    "\n",
    "与 :numref:`sec_kaggle_cifar10`一样，我们可以读取整理后的含原始图像文件的数据集。\n"
   ],
   "id": "58f4c8c8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ab4f4531",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 5783,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T11:12:46.806064Z",
     "start_time": "2023-11-03T11:12:46.704540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
    "\n",
    "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_test) for folder in ['valid', 'test']]"
   ],
   "id": "ab4f4531"
  },
  {
   "cell_type": "code",
   "source": [
    "# train_ds_swin, train_valid_ds_swin = [torchvision.datasets.ImageFolder(\n",
    "#     os.path.join(data_dir, 'train_valid_test', folder),\n",
    "#     transform=transform_train_swin) for folder in ['train', 'train_valid']]\n",
    "\n",
    "# valid_ds_swin, test_ds_swin = [torchvision.datasets.ImageFolder(\n",
    "#     os.path.join(data_dir, 'train_valid_test', folder),\n",
    "#     transform=transform_test_swin) for folder in ['valid', 'test']]"
   ],
   "metadata": {
    "id": "X02q7iOg5G-d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 10,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    }
   },
   "id": "X02q7iOg5G-d",
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1698996643795,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     },
     "user_tz": -480
    },
    "id": "LwoKDkEBRKs3",
    "outputId": "b8bda0c9-6189-48c4-ca6d-628ea8617172",
    "ExecuteTime": {
     "end_time": "2023-11-03T11:13:07.333059Z",
     "start_time": "2023-11-03T11:13:07.317264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(9502, 10222, 10357, torch.Size([3, 224, 224]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(train_valid_ds), len(test_ds), train_ds[0][0].shape"
   ],
   "id": "LwoKDkEBRKs3"
  },
  {
   "cell_type": "code",
   "source": [
    "# len(train_ds_swin), len(train_valid_ds_swin),len(test_ds_swin)"
   ],
   "metadata": {
    "id": "g5EbSon55PnZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 9,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    }
   },
   "id": "g5EbSon55PnZ",
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b12526f2"
   },
   "source": [
    "下面我们创建数据加载器实例的方式与 :numref:`sec_kaggle_cifar10`相同。\n"
   ],
   "id": "b12526f2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fcc78330",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 9,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T07:34:10.425963Z",
     "start_time": "2023-11-03T07:34:10.410763Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
    "    dataset, batch_size, shuffle=True, drop_last=True)\n",
    "    for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n",
    "                                         drop_last=True)\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n",
    "                                        drop_last=False)"
   ],
   "id": "fcc78330"
  },
  {
   "cell_type": "code",
   "source": [
    "# train_iter_swin, train_valid_iter_swin = [torch.utils.data.DataLoader(\n",
    "#     dataset, batch_size, shuffle=True, drop_last=True)\n",
    "#     for dataset in (train_ds_swin, train_valid_ds_swin)]\n",
    "\n",
    "# valid_iter_swin = torch.utils.data.DataLoader(valid_ds_swin, batch_size, shuffle=False,\n",
    "#                                          drop_last=True)\n",
    "\n",
    "# test_iter_swin = torch.utils.data.DataLoader(test_ds_swin, batch_size, shuffle=False,\n",
    "#                                         drop_last=False)"
   ],
   "metadata": {
    "id": "vTXBnUbB5SsU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 9,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    }
   },
   "id": "vTXBnUbB5SsU",
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abae5882"
   },
   "source": [
    "## [**微调预训练模型**]\n",
    "\n",
    "同样，本次比赛的数据集是ImageNet数据集的子集。\n",
    "因此，我们可以使用 :numref:`sec_fine_tuning`中讨论的方法在完整ImageNet数据集上选择预训练的模型，然后使用该模型提取图像特征，以便将其输入到定制的小规模输出网络中。\n",
    "深度学习框架的高级API提供了在ImageNet数据集上预训练的各种模型。\n",
    "在这里，我们选择预训练的ResNet-34模型，我们只需重复使用此模型的输出层（即提取的特征）的输入。\n",
    "然后，我们可以用一个可以训练的小型自定义输出网络替换原始输出层，例如堆叠两个完全连接的图层。\n",
    "与 :numref:`sec_fine_tuning`中的实验不同，以下内容不重新训练用于特征提取的预训练模型，这节省了梯度下降的时间和内存空间。\n",
    "\n",
    "回想一下，我们使用三个RGB通道的均值和标准差来对完整的ImageNet数据集进行图像标准化。\n",
    "事实上，这也符合ImageNet上预训练模型的标准化操作。\n"
   ],
   "id": "abae5882"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a0d5f683",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 9,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T11:13:13.629882Z",
     "start_time": "2023-11-03T11:13:13.546992Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_net(devices):\n",
    "    finetune_net = nn.Sequential()\n",
    "    finetune_net.features = torchvision.models.resnet152(pretrained=True)\n",
    "    # 定义一个新的输出网络，共有120个输出类别\n",
    "    finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(256, 120))\n",
    "    # 将模型参数分配给用于计算的CPU或GPU\n",
    "    finetune_net = finetune_net.to(devices[0])\n",
    "    # 冻结参数\n",
    "    for param in finetune_net.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return finetune_net\n",
    "\n",
    "\n",
    "from torchvision.models import ResNet152_Weights, ViT_B_16_Weights, ViT_L_32_Weights, Swin_V2_B_Weights\n",
    "\n",
    "\n",
    "def get_vit(devices):\n",
    "    finetune_net = nn.Sequential()\n",
    "    finetune_net.features = torchvision.models.vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # 将模型参数分配给用于计算的CPU或GPU\n",
    "    finetune_net = finetune_net.to(devices[0])\n",
    "    # 冻结参数\n",
    "    for param in finetune_net.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    finetune_net[0].heads = nn.Sequential(nn.Linear(768, 256),\n",
    "                                          nn.ReLU(), nn.Dropout(0.5),\n",
    "                                          nn.Linear(256, 120))\n",
    "    return finetune_net\n",
    "\n",
    "\n",
    "def get_vitl(devices):\n",
    "    finetune_net = nn.Sequential()\n",
    "    finetune_net.features = torchvision.models.vit_l_32(weights=ViT_L_32_Weights.IMAGENET1K_V1)\n",
    "    finetune_net.features.heads = nn.Sequential(nn.Linear(1024, 2048),\n",
    "                                                nn.ReLU(), nn.Dropout(0.5),\n",
    "                                                nn.Linear(2048, 1024),\n",
    "                                                nn.ReLU(), nn.Dropout(0.5),\n",
    "                                                nn.Linear(1024, 1024),\n",
    "                                                nn.ReLU(), nn.Dropout(0.5),\n",
    "                                                nn.Linear(1024, 1024),\n",
    "                                                nn.ReLU(), nn.Dropout(0.5),\n",
    "                                                nn.Linear(1024, 1024),\n",
    "                                                nn.ReLU(), nn.Dropout(0.5),\n",
    "                                                nn.Linear(1024, 1024),\n",
    "                                                nn.ReLU(), nn.Dropout(0.5),\n",
    "                                                nn.Linear(1024, 1024),\n",
    "                                                nn.ReLU(), nn.Dropout(0.5),\n",
    "                                                nn.Linear(1024, 256),\n",
    "                                                nn.ReLU(), nn.Dropout(0.5),\n",
    "                                                nn.Linear(256, 120))\n",
    "    # 将模型参数分配给用于计算的CPU或GPU\n",
    "    finetune_net = finetune_net.to(devices[0])\n",
    "    # 冻结参数\n",
    "    for param in finetune_net.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in finetune_net.features.heads.parameters():\n",
    "        param.requires_grad = True\n",
    "    return finetune_net\n",
    "\n",
    "\n",
    "def get_swin_B(devices):\n",
    "    finetune_net = nn.Sequential()\n",
    "    finetune_net.features = torchvision.models.swin_v2_b(weights=Swin_V2_B_Weights.IMAGENET1K_V1)\n",
    "    finetune_net.features.head = nn.Sequential(nn.Linear(1024, 256),\n",
    "                                               nn.ReLU(), nn.Dropout(0.5),\n",
    "                                               nn.Linear(256, 120))\n",
    "    # finetune_net.features.new_out = nn.Sequential(nn.Linear(1024, 256),\n",
    "    #                                       nn.ReLU(), nn.Dropout(0.5),\n",
    "    #                                       nn.Linear(256, 120))\n",
    "    # 将模型参数分配给用于计算的CPU或GPU\n",
    "    finetune_net = finetune_net.to(devices[0])\n",
    "    # 冻结参数\n",
    "    for param in finetune_net.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in finetune_net.features.head.parameters():\n",
    "        param.requires_grad = True\n",
    "    return finetune_net\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, patch_size, channels, hidden_dim, num_heads, num_layers, num_classes):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding = nn.Linear(patch_size * patch_size * channels, hidden_dim)\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            torch.randn(1, (224 // patch_size) * (224 // patch_size) + 1, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=256,\n",
    "            nhead=8,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1,\n",
    "            activation=nn.functional.relu,\n",
    "            layer_norm_eps=1e-5,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # self.test = nn.Transformer()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        patches = patches.contiguous().view(x.shape[0], -1, self.patch_size * self.patch_size * x.shape[1])\n",
    "\n",
    "        # Embed patches\n",
    "        x = self.embedding(patches)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.positional_embedding[:, :-1]\n",
    "        cls_tokens = self.cls_token.repeat(x.shape[0], 1, 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.positional_embedding\n",
    "\n",
    "        # Pass through transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Classifier on the CLS token\n",
    "        x = self.fc(x[:, 0])\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_myvit():\n",
    "    net = VisionTransformer(16, 3, 512, 8, 4, 120)\n",
    "    return net\n",
    "\n",
    "\n",
    "class VisionTransformer_conv(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, channels, hidden_dim, num_heads, dim_feedforward, num_layers, num_classes):\n",
    "        super(VisionTransformer_conv, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "\n",
    "        # 使用卷积层代替线性层以直接从图像中提取patch\n",
    "        self.patch_embed = nn.Conv2d(channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=0.1,\n",
    "            activation=nn.functional.relu,\n",
    "            layer_norm_eps=1e-5,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 使用卷积层提取patch并展平\n",
    "        x = self.patch_embed(x)  # [B, C, H, W] -> [B, hidden_dim, H/P, W/P]\n",
    "        x = x.flatten(2)  # [B, hidden_dim, H/P * W/P]\n",
    "        x = x.transpose(1, 2)  # [B, H/P * W/P, hidden_dim]\n",
    "\n",
    "        # Add positional embedding\n",
    "        cls_tokens = self.cls_token.repeat(x.shape[0], 1, 1)  # [B, 1, hidden_dim]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [B, 1 + H/P * W/P, hidden_dim]\n",
    "        x = x + self.positional_embedding  # [B, 1 + H/P * W/P, hidden_dim]\n",
    "\n",
    "        # Pass through transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Classifier on the CLS token\n",
    "        x = self.fc(x[:, 0])\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_vit_conv():\n",
    "    net = VisionTransformer_conv(224, 16, 3, 256, 4, 512, 3, 120)\n",
    "    return net\n",
    "\n",
    "\n",
    "import clip\n",
    "\n",
    "\n",
    "def get_clip():\n",
    "    model, preprocess = clip.load('ViT-B/32', device=torch.device('mps'))\n",
    "    return model, preprocess"
   ],
   "id": "a0d5f683"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: a dog\n",
      "[[1.114e-03 9.971e-01 4.573e-04 9.093e-04]]\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = get_clip()\n",
    "device = torch.device('mps')\n",
    "from PIL import Image\n",
    "\n",
    "image = preprocess(Image.open(\n",
    "    '../data/dog-breed-identification/train_valid_test/test/unknown/0a0b97441050bba8e733506de4655ea1.jpg')).unsqueeze(\n",
    "    0).to(device)\n",
    "labels = [\"a diagram\", \"a dog\", 'a affenpinscher', \"a cat\"]\n",
    "text = clip.tokenize(labels).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "print(\"Label probs:\", labels[probs.argmax()])\n",
    "print(probs)\n",
    "# preds = []\n",
    "# for data, label in test_iter:\n",
    "#     output = torch.nn.functional.softmax(net4(data.to(devices[0])), dim=1)\n",
    "#     preds.extend(output.cpu().detach().numpy())\n",
    "# ids = sorted(os.listdir(\n",
    "#     os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\n",
    "# \n",
    "# with open('./dog/submission.csv', 'w') as f:\n",
    "#     f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n",
    "#     for i, output in zip(ids, preds):\n",
    "#         f.write(i.split('.')[0] + ',' + ','.join(\n",
    "#             [str(num) for num in output]) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T11:19:10.209514Z",
     "start_time": "2023-11-03T11:19:08.409124Z"
    }
   },
   "id": "62cfa18c632106e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab327251"
   },
   "source": [
    "在[**计算损失**]之前，我们首先获取预训练模型的输出层的输入，即提取的特征。\n",
    "然后我们使用此特征作为我们小型自定义输出网络的输入来计算损失。\n"
   ],
   "id": "ab327251"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9cc9cdc0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 9,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T07:34:21.786959Z",
     "start_time": "2023-11-03T07:34:21.769436Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "\n",
    "def evaluate_loss(data_iter, net, devices):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for features, labels in data_iter:\n",
    "        features, labels = features.to(devices[0]), labels.to(devices[0])\n",
    "        outputs = net(features)\n",
    "        l = loss(outputs, labels)\n",
    "        l_sum += l.sum()\n",
    "        n += labels.numel()\n",
    "    return (l_sum / n).to('cpu')"
   ],
   "id": "9cc9cdc0"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mIlcZ3fCqS9R",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 9,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T07:34:22.936547Z",
     "start_time": "2023-11-03T07:34:22.923054Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ],
   "id": "mIlcZ3fCqS9R"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "110159c7"
   },
   "source": [
    "## 定义[**训练函数**]\n",
    "\n",
    "我们将根据模型在验证集上的表现选择模型并调整超参数。\n",
    "模型训练函数`train`只迭代小型自定义输出网络的参数。\n"
   ],
   "id": "110159c7"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5678b941",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996643795,
     "user_tz": -480,
     "elapsed": 9,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T07:57:03.213209Z",
     "start_time": "2023-11-03T07:57:03.202418Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "          lr_decay):\n",
    "    # 只训练小型自定义输出网络\n",
    "    # net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    net = net.to(devices[0])\n",
    "    trainer = torch.optim.SGD((param for param in net.parameters()\n",
    "                               if param.requires_grad), lr=lr,\n",
    "                              momentum=0.9, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid loss')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for features, labels in tqdm(train_iter):\n",
    "            timer.start()\n",
    "            features, labels = features.to(devices[0]), labels.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            output = net(features)\n",
    "            l = loss(output, labels).sum()\n",
    "            # l.requires_grad = True\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(l, labels.shape[0])\n",
    "            timer.stop()\n",
    "        measures = f'{epoch}/{num_epochs} train loss {metric[0] / metric[1]:.3f}'\n",
    "        if valid_iter is not None:\n",
    "            valid_loss = evaluate_loss(valid_iter, net, devices)\n",
    "            measures += f', valid loss {valid_loss:.3f}'\n",
    "        scheduler.step()\n",
    "        print(measures + f',{output.shape}')\n",
    "    torch.save(net.state_dict(), '/content/drive/MyDrive/models/dog_classifier.ckpt')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid loss {valid_loss:.3f}'\n",
    "    print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n",
    "                     f' examples/sec on {str(devices)}')"
   ],
   "id": "5678b941"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09725612"
   },
   "source": [
    "## [**训练和验证模型**]\n",
    "\n",
    "现在我们可以训练和验证模型了，以下超参数都是可调的。\n",
    "例如，我们可以增加迭代轮数。\n",
    "另外，由于`lr_period`和`lr_decay`分别设置为2和0.9，\n",
    "因此优化算法的学习速率将在每2个迭代后乘以0.9。\n"
   ],
   "id": "09725612"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5bcd4b0a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996644285,
     "user_tz": -480,
     "elapsed": 499,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-03T08:07:20.008410Z",
     "start_time": "2023-11-03T08:07:19.989947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_embedding requires_grad=True\n",
      "cls_token requires_grad=True\n",
      "patch_embed.weight requires_grad=True\n",
      "patch_embed.bias requires_grad=True\n",
      "transformer.layers.0.self_attn.in_proj_weight requires_grad=True\n",
      "transformer.layers.0.self_attn.in_proj_bias requires_grad=True\n",
      "transformer.layers.0.self_attn.out_proj.weight requires_grad=True\n",
      "transformer.layers.0.self_attn.out_proj.bias requires_grad=True\n",
      "transformer.layers.0.linear1.weight requires_grad=True\n",
      "transformer.layers.0.linear1.bias requires_grad=True\n",
      "transformer.layers.0.linear2.weight requires_grad=True\n",
      "transformer.layers.0.linear2.bias requires_grad=True\n",
      "transformer.layers.0.norm1.weight requires_grad=True\n",
      "transformer.layers.0.norm1.bias requires_grad=True\n",
      "transformer.layers.0.norm2.weight requires_grad=True\n",
      "transformer.layers.0.norm2.bias requires_grad=True\n",
      "transformer.layers.1.self_attn.in_proj_weight requires_grad=True\n",
      "transformer.layers.1.self_attn.in_proj_bias requires_grad=True\n",
      "transformer.layers.1.self_attn.out_proj.weight requires_grad=True\n",
      "transformer.layers.1.self_attn.out_proj.bias requires_grad=True\n",
      "transformer.layers.1.linear1.weight requires_grad=True\n",
      "transformer.layers.1.linear1.bias requires_grad=True\n",
      "transformer.layers.1.linear2.weight requires_grad=True\n",
      "transformer.layers.1.linear2.bias requires_grad=True\n",
      "transformer.layers.1.norm1.weight requires_grad=True\n",
      "transformer.layers.1.norm1.bias requires_grad=True\n",
      "transformer.layers.1.norm2.weight requires_grad=True\n",
      "transformer.layers.1.norm2.bias requires_grad=True\n",
      "transformer.layers.2.self_attn.in_proj_weight requires_grad=True\n",
      "transformer.layers.2.self_attn.in_proj_bias requires_grad=True\n",
      "transformer.layers.2.self_attn.out_proj.weight requires_grad=True\n",
      "transformer.layers.2.self_attn.out_proj.bias requires_grad=True\n",
      "transformer.layers.2.linear1.weight requires_grad=True\n",
      "transformer.layers.2.linear1.bias requires_grad=True\n",
      "transformer.layers.2.linear2.weight requires_grad=True\n",
      "transformer.layers.2.linear2.bias requires_grad=True\n",
      "transformer.layers.2.norm1.weight requires_grad=True\n",
      "transformer.layers.2.norm1.bias requires_grad=True\n",
      "transformer.layers.2.norm2.weight requires_grad=True\n",
      "transformer.layers.2.norm2.bias requires_grad=True\n",
      "fc.weight requires_grad=True\n",
      "fc.bias requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "devices, num_epochs, lr, wd = [torch.device('mps')], 50, 1e-4, 1e-4\n",
    "lr_period, lr_decay, = 2, 0.9\n",
    "# net1=get_vit(devices)\n",
    "# net2 =  get_vitl(devices)\n",
    "# net3 =  get_swin_B(devices)\n",
    "net4 = get_vit_conv()\n",
    "for name, param in net4.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} requires_grad={param.requires_grad}\")"
   ],
   "id": "5bcd4b0a"
  },
  {
   "cell_type": "code",
   "source": [
    "# for name, param in net3.named_parameters():\n",
    "#     print(f\"{name} requires_grad={param.requires_grad}\")\n",
    "torch.__version__"
   ],
   "metadata": {
    "id": "6vXRxsLa6IR-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698996644285,
     "user_tz": -480,
     "elapsed": 7,
     "user": {
      "displayName": "Lingfeng Ren",
      "userId": "17783286971809197165"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "5aeeac23-094f-4ce6-e5de-41e9fe8102d7",
    "ExecuteTime": {
     "end_time": "2023-11-03T07:57:36.987903Z",
     "start_time": "2023-11-03T07:57:36.972851Z"
    }
   },
   "id": "6vXRxsLa6IR-",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.1.0'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PAZlDq2bSDkN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b9c33879-7e13-45dd-b8b4-4a23603afc57",
    "ExecuteTime": {
     "end_time": "2023-11-03T08:04:53.413233Z",
     "start_time": "2023-11-03T07:58:00.028478Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:42<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50 train loss 4.849, valid loss 4.847,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:43<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50 train loss 4.799, valid loss 4.711,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:44<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/50 train loss 4.751, valid loss 4.681,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:44<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/50 train loss 4.721, valid loss 4.654,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:45<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/50 train loss 4.702, valid loss 4.629,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:43<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/50 train loss 4.685, valid loss 4.635,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:44<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/50 train loss 4.668, valid loss 4.562,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:45<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/50 train loss 4.653, valid loss 4.605,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 55/74 [00:37<00:12,  1.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet4\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_period\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m      \u001B[49m\u001B[43mlr_decay\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 17\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period, lr_decay)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m     16\u001B[0m     metric \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mAccumulator(\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m features, labels \u001B[38;5;129;01min\u001B[39;00m tqdm(train_iter):\n\u001B[1;32m     18\u001B[0m         timer\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     19\u001B[0m         features, labels \u001B[38;5;241m=\u001B[39m features\u001B[38;5;241m.\u001B[39mto(devices[\u001B[38;5;241m0\u001B[39m]), labels\u001B[38;5;241m.\u001B[39mto(devices[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/tqdm/std.py:1182\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1179\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1182\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torchvision/datasets/folder.py:231\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    229\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(path)\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 231\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1281\u001B[0m, in \u001B[0;36mColorJitter.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m   1279\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fn_id \u001B[38;5;129;01min\u001B[39;00m fn_idx:\n\u001B[1;32m   1280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m brightness_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1281\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_brightness\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbrightness_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1282\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m contrast_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1283\u001B[0m         img \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39madjust_contrast(img, contrast_factor)\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torchvision/transforms/functional.py:893\u001B[0m, in \u001B[0;36madjust_brightness\u001B[0;34m(img, brightness_factor)\u001B[0m\n\u001B[1;32m    891\u001B[0m     _log_api_usage_once(adjust_brightness)\n\u001B[1;32m    892\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 893\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_brightness\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbrightness_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39madjust_brightness(img, brightness_factor)\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:73\u001B[0m, in \u001B[0;36madjust_brightness\u001B[0;34m(img, brightness_factor)\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg should be PIL Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(img)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     72\u001B[0m enhancer \u001B[38;5;241m=\u001B[39m ImageEnhance\u001B[38;5;241m.\u001B[39mBrightness(img)\n\u001B[0;32m---> 73\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43menhancer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menhance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbrightness_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/PIL/ImageEnhance.py:36\u001B[0m, in \u001B[0;36m_Enhance.enhance\u001B[0;34m(self, factor)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21menhance\u001B[39m(\u001B[38;5;28mself\u001B[39m, factor):\n\u001B[1;32m     26\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124;03m    Returns an enhanced image.\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;03m    :rtype: :py:class:`~PIL.Image.Image`\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdegenerate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfactor\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/PIL/Image.py:3347\u001B[0m, in \u001B[0;36mblend\u001B[0;34m(im1, im2, alpha)\u001B[0m\n\u001B[1;32m   3345\u001B[0m im1\u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m   3346\u001B[0m im2\u001B[38;5;241m.\u001B[39mload()\n\u001B[0;32m-> 3347\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m im1\u001B[38;5;241m.\u001B[39m_new(\u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mim2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(net4, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ],
   "id": "PAZlDq2bSDkN"
  },
  {
   "cell_type": "code",
   "source": [
    "# train(net3, train_iter_swin, valid_iter_swin, num_epochs, lr, wd, devices, lr_period,\n",
    "#       lr_decay)"
   ],
   "metadata": {
    "id": "ZIGVmWYL5gic"
   },
   "id": "ZIGVmWYL5gic",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train(net3, train_valid_iter_swin, None, 10, lr, wd, devices, lr_period,\n",
    "#       lr_decay)"
   ],
   "metadata": {
    "id": "fYAEa0RkAeFb"
   },
   "id": "fYAEa0RkAeFb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train(net4, train_valid_iter, None, 10, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ],
   "metadata": {
    "id": "oLVCKSuoH-TE",
    "ExecuteTime": {
     "end_time": "2023-11-03T07:54:11.768273Z",
     "start_time": "2023-11-03T07:45:44.170287Z"
    }
   },
   "id": "oLVCKSuoH-TE",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:37<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10 train loss 0.696,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:35<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 train loss 0.673,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:36<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 train loss 0.630,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:37<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 train loss 0.649,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:37<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/10 train loss 0.639,torch.Size([128, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 18/79 [00:22<01:16,  1.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_valid_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_period\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m      \u001B[49m\u001B[43mlr_decay\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[13], line 17\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period, lr_decay)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m     16\u001B[0m     metric \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mAccumulator(\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m features, labels \u001B[38;5;129;01min\u001B[39;00m tqdm(train_iter):\n\u001B[1;32m     18\u001B[0m         timer\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     19\u001B[0m         features, labels \u001B[38;5;241m=\u001B[39m features\u001B[38;5;241m.\u001B[39mto(devices[\u001B[38;5;241m0\u001B[39m]), labels\u001B[38;5;241m.\u001B[39mto(devices[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/tqdm/std.py:1182\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1179\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1182\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torchvision/datasets/folder.py:229\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m    index (int): Index\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    228\u001B[0m path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[index]\n\u001B[0;32m--> 229\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    231\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torchvision/datasets/folder.py:268\u001B[0m, in \u001B[0;36mdefault_loader\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torchvision/datasets/folder.py:248\u001B[0m, in \u001B[0;36mpil_loader\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    247\u001B[0m     img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(f)\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/PIL/Image.py:916\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mPalette\u001B[38;5;241m.\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[1;32m    870\u001B[0m ):\n\u001B[1;32m    871\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    872\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[1;32m    873\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 916\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    918\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    920\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/PIL/ImageFile.py:249\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 249\u001B[0m         s \u001B[38;5;241m=\u001B[39m \u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecodermaxblock\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    250\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mIndexError\u001B[39;00m, struct\u001B[38;5;241m.\u001B[39merror) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;66;03m# truncated png/gif\u001B[39;00m\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:414\u001B[0m, in \u001B[0;36mJpegImageFile.load_read\u001B[0;34m(self, read_bytes)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_read\u001B[39m(\u001B[38;5;28mself\u001B[39m, read_bytes):\n\u001B[1;32m    409\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;124;03m    internal: read more image data\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;124;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001B[39;00m\n\u001B[1;32m    412\u001B[0m \u001B[38;5;124;03m    so libjpeg can finish decoding\u001B[39;00m\n\u001B[1;32m    413\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 414\u001B[0m     s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mread_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m ImageFile\u001B[38;5;241m.\u001B[39mLOAD_TRUNCATED_IMAGES \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_ended\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    417\u001B[0m         \u001B[38;5;66;03m# Premature EOF.\u001B[39;00m\n\u001B[1;32m    418\u001B[0m         \u001B[38;5;66;03m# Pretend file is finished adding EOI marker\u001B[39;00m\n\u001B[1;32m    419\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ended \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53452ac9"
   },
   "source": [
    "## [**对测试集分类**]并在Kaggle提交结果\n",
    "\n",
    "与 :numref:`sec_kaggle_cifar10`中的最后一步类似，最终所有标记的数据（包括验证集）都用于训练模型和对测试集进行分类。\n",
    "我们将使用训练好的自定义输出网络进行分类。\n"
   ],
   "id": "53452ac9"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "933ae1ca",
    "ExecuteTime": {
     "end_time": "2023-11-03T08:05:46.082408Z",
     "start_time": "2023-11-03T08:05:03.180303Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dog/submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m     preds\u001B[38;5;241m.\u001B[39mextend(output\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m      8\u001B[0m ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(os\u001B[38;5;241m.\u001B[39mlistdir(\n\u001B[1;32m      9\u001B[0m     os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_valid_test\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124munknown\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/dog/submission.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mw\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     11\u001B[0m     f\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid,\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(train_valid_ds\u001B[38;5;241m.\u001B[39mclasses) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(ids, preds):\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    284\u001B[0m     )\n\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/dog/submission.csv'"
     ]
    }
   ],
   "source": [
    "# net2 = get_vitl(devices)\n",
    "\n",
    "\n",
    "preds = []\n",
    "for data, label in test_iter:\n",
    "    output = torch.nn.functional.softmax(net4(data.to(devices[0])), dim=1)\n",
    "    preds.extend(output.cpu().detach().numpy())\n",
    "ids = sorted(os.listdir(\n",
    "    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\n"
   ],
   "id": "933ae1ca"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "fe3CflVeBPF9",
    "ExecuteTime": {
     "end_time": "2023-11-03T08:06:08.091513Z",
     "start_time": "2023-11-03T08:06:07.753466Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./dog/submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ],
   "id": "fe3CflVeBPF9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32f69a07"
   },
   "source": [
    "上面的代码将生成一个`submission.csv`文件，以 :numref:`sec_kaggle_house`中描述的方式提在Kaggle上提交。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* ImageNet数据集中的图像比CIFAR-10图像尺寸大，我们可能会修改不同数据集上任务的图像增广操作。\n",
    "* 要对ImageNet数据集的子集进行分类，我们可以利用完整ImageNet数据集上的预训练模型来提取特征并仅训练小型自定义输出网络，这将减少计算时间和节省内存空间。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 试试使用完整Kaggle比赛数据集，增加`batch_size`（批量大小）和`num_epochs`（迭代轮数），或者设计其它超参数为`lr = 0.01`，`lr_period = 10`，和`lr_decay = 0.1`时，能取得什么结果？\n",
    "1. 如果使用更深的预训练模型，会得到更好的结果吗？如何调整超参数？能进一步改善结果吗？\n"
   ],
   "id": "32f69a07"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac4d2a2b"
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2833)\n"
   ],
   "id": "ac4d2a2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f50db1dfd294b5c2"
   },
   "outputs": [],
   "source": [],
   "id": "f50db1dfd294b5c2"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuType": "V100"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
