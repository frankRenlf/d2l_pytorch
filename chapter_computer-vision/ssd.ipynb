{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8bed8a",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "\n",
    "# 单发多框检测（SSD）\n",
    ":label:`sec_ssd`\n",
    "\n",
    "在 :numref:`sec_bbox`— :numref:`sec_object-detection-dataset`中，我们分别介绍了边界框、锚框、多尺度目标检测和用于目标检测的数据集。\n",
    "现在我们已经准备好使用这样的背景知识来设计一个目标检测模型：单发多框检测（SSD） :cite:`Liu.Anguelov.Erhan.ea.2016`。\n",
    "该模型简单、快速且被广泛使用。尽管这只是其中一种目标检测模型，但本节中的一些设计原则和实现细节也适用于其他模型。\n",
    "\n",
    "## 模型\n",
    "\n",
    " :numref:`fig_ssd`描述了单发多框检测模型的设计。\n",
    "此模型主要由基础网络组成，其后是几个多尺度特征块。\n",
    "基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。\n",
    "单发多框检测论文中选用了在分类层之前截断的VGG :cite:`Liu.Anguelov.Erhan.ea.2016`，现在也常用ResNet替代。\n",
    "我们可以设计基础网络，使它输出的高和宽较大。\n",
    "这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。\n",
    "接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。\n",
    "\n",
    "回想一下在 :numref:`sec_multiscale-object-detection`中，通过深度神经网络分层表示图像的多尺度目标检测的设计。\n",
    "由于接近 :numref:`fig_ssd`顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。\n",
    "简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。\n",
    "\n",
    "![单发多框检测模型主要由一个基础网络块和若干多尺度特征块串联而成。](../img/ssd.svg)\n",
    ":label:`fig_ssd`\n",
    "\n",
    "在下面，我们将介绍 :numref:`fig_ssd`中不同块的实施细节。\n",
    "首先，我们将讨论如何实施类别和边界框预测。\n",
    "\n",
    "### [**类别预测层**]\n",
    "\n",
    "设目标类别的数量为$q$。这样一来，锚框有$q+1$个类别，其中0类是背景。\n",
    "在某个尺度下，设特征图的高和宽分别为$h$和$w$。\n",
    "如果以其中每个单元为中心生成$a$个锚框，那么我们需要对$hwa$个锚框进行分类。\n",
    "如果使用全连接层作为输出，很容易导致模型参数过多。\n",
    "回忆 :numref:`sec_nin`一节介绍的使用卷积层的通道来输出类别预测的方法，\n",
    "单发多框检测采用同样的方法来降低模型复杂度。\n",
    "\n",
    "具体来说，类别预测层使用一个保持输入高和宽的卷积层。\n",
    "这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。\n",
    "考虑输出和输入同一空间坐标（$x$、$y$）：输出特征图上（$x$、$y$）坐标的通道里包含了以输入特征图（$x$、$y$）坐标为中心生成的所有锚框的类别预测。\n",
    "因此输出通道数为$a(q+1)$，其中索引为$i(q+1) + j$（$0 \\leq j \\leq q$）的通道代表了索引为$i$的锚框有关类别索引为$j$的预测。\n",
    "\n",
    "在下面，我们定义了这样一个类别预测层，通过参数`num_anchors`和`num_classes`分别指定了$a$和$q$。\n",
    "该图层使用填充为1的$3\\times3$的卷积层。此卷积层的输入和输出的宽度和高度保持不变。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "342ee008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:17.809326Z",
     "iopub.status.busy": "2022-12-07T17:38:17.808709Z",
     "iopub.status.idle": "2022-12-07T17:38:20.859771Z",
     "shell.execute_reply": "2022-12-07T17:38:20.858953Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.364106Z",
     "start_time": "2023-08-21T16:30:25.281813Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def cls_predictor(num_inputs, num_anchors, num_classes):\n",
    "    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),\n",
    "                     kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd47b9b",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "### (**边界框预测层**)\n",
    "\n",
    "边界框预测层的设计与类别预测层的设计类似。\n",
    "唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是$q+1$个类别。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "874f3567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.863567Z",
     "iopub.status.busy": "2022-12-07T17:38:20.863024Z",
     "iopub.status.idle": "2022-12-07T17:38:20.867079Z",
     "shell.execute_reply": "2022-12-07T17:38:20.866347Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.364426Z",
     "start_time": "2023-08-21T16:30:25.285059Z"
    }
   },
   "outputs": [],
   "source": [
    "def bbox_predictor(num_inputs, num_anchors):\n",
    "    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621251f5",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "### [**连结多尺度的预测**]\n",
    "\n",
    "正如我们所提到的，单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。\n",
    "在不同的尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。\n",
    "因此，不同尺度下预测输出的形状可能会有所不同。\n",
    "\n",
    "在以下示例中，我们为同一个小批量构建两个不同比例（`Y1`和`Y2`）的特征图，其中`Y2`的高度和宽度是`Y1`的一半。\n",
    "以类别预测为例，假设`Y1`和`Y2`的每个单元分别生成了$5$个和$3$个锚框。\n",
    "进一步假设目标类别的数量为$10$，对于特征图`Y1`和`Y2`，类别预测输出中的通道数分别为$5\\times(10+1)=55$和$3\\times(10+1)=33$，其中任一输出的形状是（批量大小，通道数，高度，宽度）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "db0024c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.870137Z",
     "iopub.status.busy": "2022-12-07T17:38:20.869726Z",
     "iopub.status.idle": "2022-12-07T17:38:20.904646Z",
     "shell.execute_reply": "2022-12-07T17:38:20.903941Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.378629Z",
     "start_time": "2023-08-21T16:30:25.288654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(x, block):\n",
    "    return block(x)\n",
    "\n",
    "Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\n",
    "Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\n",
    "Y1.shape, Y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f6da6",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "正如我们所看到的，除了批量大小这一维度外，其他三个维度都具有不同的尺寸。\n",
    "为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。\n",
    "\n",
    "通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。\n",
    "因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的（批量大小，高$\\times$宽$\\times$通道数）的格式，以方便之后在维度$1$上的连结。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c6bc9a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.907950Z",
     "iopub.status.busy": "2022-12-07T17:38:20.907521Z",
     "iopub.status.idle": "2022-12-07T17:38:20.911886Z",
     "shell.execute_reply": "2022-12-07T17:38:20.911179Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.380274Z",
     "start_time": "2023-08-21T16:30:25.293238Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten_pred(pred):\n",
    "    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)\n",
    "\n",
    "def concat_preds(preds):\n",
    "    return torch.cat([flatten_pred(p) for p in preds], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369427b3",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "这样一来，尽管`Y1`和`Y2`在通道数、高度和宽度方面具有不同的大小，我们仍然可以在同一个小批量的两个不同尺度上连接这两个预测输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f1050eeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.914994Z",
     "iopub.status.busy": "2022-12-07T17:38:20.914499Z",
     "iopub.status.idle": "2022-12-07T17:38:20.920406Z",
     "shell.execute_reply": "2022-12-07T17:38:20.919655Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.429021Z",
     "start_time": "2023-08-21T16:30:25.295109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 25300])"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_preds([Y1, Y2]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c7176",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "### [**高和宽减半块**]\n",
    "\n",
    "为了在多个尺度下检测目标，我们在下面定义了高和宽减半块`down_sample_blk`，该模块将输入特征图的高度和宽度减半。\n",
    "事实上，该块应用了在 :numref:`subsec_vgg-blocks`中的VGG模块设计。\n",
    "更具体地说，每个高和宽减半块由两个填充为$1$的$3\\times3$的卷积层、以及步幅为$2$的$2\\times2$最大汇聚层组成。\n",
    "我们知道，填充为$1$的$3\\times3$卷积层不改变特征图的形状。但是，其后的$2\\times2$的最大汇聚层将输入特征图的高度和宽度减少了一半。\n",
    "对于此高和宽减半块的输入和输出特征图，因为$1\\times 2+(3-1)+(3-1)=6$，所以输出中的每个单元在输入上都有一个$6\\times6$的感受野。因此，高和宽减半块会扩大每个单元在其输出特征图中的感受野。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2aa3380b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.923772Z",
     "iopub.status.busy": "2022-12-07T17:38:20.923154Z",
     "iopub.status.idle": "2022-12-07T17:38:20.928055Z",
     "shell.execute_reply": "2022-12-07T17:38:20.927340Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.432487Z",
     "start_time": "2023-08-21T16:30:25.299618Z"
    }
   },
   "outputs": [],
   "source": [
    "def down_sample_blk(in_channels, out_channels):\n",
    "    blk = []\n",
    "    for _ in range(2):\n",
    "        blk.append(nn.Conv2d(in_channels, out_channels,\n",
    "                             kernel_size=3, padding=1))\n",
    "        blk.append(nn.BatchNorm2d(out_channels))\n",
    "        blk.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    blk.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4144b4",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "在以下示例中，我们构建的高和宽减半块会更改输入通道的数量，并将输入特征图的高度和宽度减半。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9be135c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.931332Z",
     "iopub.status.busy": "2022-12-07T17:38:20.930734Z",
     "iopub.status.idle": "2022-12-07T17:38:20.940465Z",
     "shell.execute_reply": "2022-12-07T17:38:20.939746Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.442699Z",
     "start_time": "2023-08-21T16:30:25.302054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 10, 10, 10])"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9055d45",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "### [**基本网络块**]\n",
    "\n",
    "基本网络块用于从输入图像中抽取特征。\n",
    "为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍。\n",
    "给定输入图像的形状为$256\\times256$，此基本网络块输出的特征图形状为$32 \\times 32$（$256/2^3=32$）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0ee88bfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.943829Z",
     "iopub.status.busy": "2022-12-07T17:38:20.943214Z",
     "iopub.status.idle": "2022-12-07T17:38:20.979005Z",
     "shell.execute_reply": "2022-12-07T17:38:20.978212Z"
    },
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.519342Z",
     "start_time": "2023-08-21T16:30:25.307593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 64, 32, 32])"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def base_net():\n",
    "    blk = []\n",
    "    num_filters = [3, 16, 32, 64]\n",
    "    for i in range(len(num_filters) - 1):\n",
    "        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "forward(torch.zeros((2, 3, 256, 256)), base_net()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f120c",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "### 完整的模型\n",
    "\n",
    "[**完整的单发多框检测模型由五个模块组成**]。每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是基本网络块，第二个到第四个是高和宽减半块，最后一个模块使用全局最大池将高度和宽度都降到1。从技术上讲，第二到第五个区块都是 :numref:`fig_ssd`中的多尺度特征块。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "630514a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.982533Z",
     "iopub.status.busy": "2022-12-07T17:38:20.981993Z",
     "iopub.status.idle": "2022-12-07T17:38:20.986626Z",
     "shell.execute_reply": "2022-12-07T17:38:20.985868Z"
    },
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.519466Z",
     "start_time": "2023-08-21T16:30:25.355769Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_blk(i):\n",
    "    if i == 0:\n",
    "        blk = base_net()\n",
    "    elif i == 1:\n",
    "        blk = down_sample_blk(64, 128)\n",
    "    elif i == 4:\n",
    "        blk = nn.AdaptiveMaxPool2d((1,1))\n",
    "    else:\n",
    "        blk = down_sample_blk(128, 128)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f94f0",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "现在我们[**为每个块定义前向传播**]。与图像分类任务不同，此处的输出包括：CNN特征图`Y`；在当前尺度下根据`Y`生成的锚框；预测的这些锚框的类别和偏移量（基于`Y`）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "826e6b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.990006Z",
     "iopub.status.busy": "2022-12-07T17:38:20.989396Z",
     "iopub.status.idle": "2022-12-07T17:38:20.993766Z",
     "shell.execute_reply": "2022-12-07T17:38:20.993051Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.519860Z",
     "start_time": "2023-08-21T16:30:25.358592Z"
    }
   },
   "outputs": [],
   "source": [
    "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
    "    Y = blk(X)\n",
    "    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n",
    "    cls_preds = cls_predictor(Y)\n",
    "    bbox_preds = bbox_predictor(Y)\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626cb87",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "回想一下，在 :numref:`fig_ssd`中，一个较接近顶部的多尺度特征块是用于检测较大目标的，因此需要生成更大的锚框。\n",
    "在上面的前向传播中，在每个多尺度特征块上，我们通过调用的`multibox_prior`函数（见 :numref:`sec_anchor`）的`sizes`参数传递两个比例值的列表。\n",
    "在下面，0.2和1.05之间的区间被均匀分成五个部分，以确定五个模块的在不同尺度下的较小值：0.2、0.37、0.54、0.71和0.88。\n",
    "之后，他们较大的值由$\\sqrt{0.2 \\times 0.37} = 0.272$、$\\sqrt{0.37 \\times 0.54} = 0.447$等给出。\n",
    "\n",
    "[~~超参数~~]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "42eb5b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:20.997040Z",
     "iopub.status.busy": "2022-12-07T17:38:20.996464Z",
     "iopub.status.idle": "2022-12-07T17:38:21.000736Z",
     "shell.execute_reply": "2022-12-07T17:38:21.000016Z"
    },
    "origin_pos": 39,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.519967Z",
     "start_time": "2023-08-21T16:30:25.361592Z"
    }
   },
   "outputs": [],
   "source": [
    "sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\n",
    "         [0.88, 0.961]]\n",
    "ratios = [[1, 2, 0.5]] * 5\n",
    "num_anchors = len(sizes[0]) + len(ratios[0]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c096208",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "现在，我们就可以按如下方式[**定义完整的模型**]`TinySSD`了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "117ebaba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:21.003960Z",
     "iopub.status.busy": "2022-12-07T17:38:21.003469Z",
     "iopub.status.idle": "2022-12-07T17:38:21.011143Z",
     "shell.execute_reply": "2022-12-07T17:38:21.010414Z"
    },
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:25.521056Z",
     "start_time": "2023-08-21T16:30:25.365065Z"
    }
   },
   "outputs": [],
   "source": [
    "class TinySSD(nn.Module):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(TinySSD, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        idx_to_in_channels = [64, 128, 128, 128, 128]\n",
    "        for i in range(5):\n",
    "            # 即赋值语句self.blk_i=get_blk(i)\n",
    "            setattr(self, f'blk_{i}', get_blk(i))\n",
    "            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],\n",
    "                                                    num_anchors, num_classes))\n",
    "            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],\n",
    "                                                      num_anchors))\n",
    "\n",
    "    def forward(self, X):\n",
    "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "        for i in range(5):\n",
    "            # getattr(self,'blk_%d'%i)即访问self.blk_i\n",
    "            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n",
    "                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n",
    "                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n",
    "        anchors = torch.cat(anchors, dim=1)\n",
    "        cls_preds = concat_preds(cls_preds)\n",
    "        cls_preds = cls_preds.reshape(\n",
    "            cls_preds.shape[0], -1, self.num_classes + 1)\n",
    "        bbox_preds = concat_preds(bbox_preds)\n",
    "        return anchors, cls_preds, bbox_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147c9d0",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "我们[**创建一个模型实例，然后使用它**]对一个$256 \\times 256$像素的小批量图像`X`(**执行前向传播**)。\n",
    "\n",
    "如本节前面部分所示，第一个模块输出特征图的形状为$32 \\times 32$。\n",
    "回想一下，第二到第四个模块为高和宽减半块，第五个模块为全局汇聚层。\n",
    "由于以特征图的每个单元为中心有$4$个锚框生成，因此在所有五个尺度下，每个图像总共生成$(32^2 + 16^2 + 8^2 + 4^2 + 1)\\times 4 = 5444$个锚框。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "06038003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:21.014471Z",
     "iopub.status.busy": "2022-12-07T17:38:21.013871Z",
     "iopub.status.idle": "2022-12-07T17:38:21.465449Z",
     "shell.execute_reply": "2022-12-07T17:38:21.464614Z"
    },
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:26.370146Z",
     "start_time": "2023-08-21T16:30:25.367176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output anchors: torch.Size([1, 5444, 4])\n",
      "output class preds: torch.Size([32, 5444, 2])\n",
      "output bbox preds: torch.Size([32, 21776])\n"
     ]
    }
   ],
   "source": [
    "net = TinySSD(num_classes=1)\n",
    "X = torch.zeros((32, 3, 256, 256))\n",
    "anchors, cls_preds, bbox_preds = net(X)\n",
    "\n",
    "print('output anchors:', anchors.shape)\n",
    "print('output class preds:', cls_preds.shape)\n",
    "print('output bbox preds:', bbox_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fcaf31",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "## 训练模型\n",
    "\n",
    "现在，我们将描述如何训练用于目标检测的单发多框检测模型。\n",
    "\n",
    "### 读取数据集和初始化\n",
    "\n",
    "首先，让我们[**读取**] :numref:`sec_object-detection-dataset`中描述的(**香蕉检测数据集**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "eb1120b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:21.469061Z",
     "iopub.status.busy": "2022-12-07T17:38:21.468491Z",
     "iopub.status.idle": "2022-12-07T17:38:25.865354Z",
     "shell.execute_reply": "2022-12-07T17:38:25.864562Z"
    },
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:28.226226Z",
     "start_time": "2023-08-21T16:30:26.370082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1000 training examples\n",
      "read 100 validation examples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_iter, _ = d2l.load_data_bananas(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90931d",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "香蕉检测数据集中，目标的类别数为1。\n",
    "定义好模型后，我们需要(**初始化其参数并定义优化算法**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a69d4488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:25.868734Z",
     "iopub.status.busy": "2022-12-07T17:38:25.868279Z",
     "iopub.status.idle": "2022-12-07T17:38:25.888687Z",
     "shell.execute_reply": "2022-12-07T17:38:25.887968Z"
    },
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:28.233449Z",
     "start_time": "2023-08-21T16:30:28.226335Z"
    }
   },
   "outputs": [],
   "source": [
    "device, net = torch.device('cpu'), TinySSD(num_classes=1)\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.02, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90eba4",
   "metadata": {
    "origin_pos": 54
   },
   "source": [
    "### [**定义损失函数和评价函数**]\n",
    "\n",
    "目标检测有两种类型的损失。\n",
    "第一种有关锚框类别的损失：我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算；\n",
    "第二种有关正类锚框偏移量的损失：预测偏移量是一个回归问题。\n",
    "但是，对于这个回归问题，我们在这里不使用 :numref:`subsec_normal_distribution_and_squared_loss`中描述的平方损失，而是使用$L_1$范数损失，即预测值和真实值之差的绝对值。\n",
    "掩码变量`bbox_masks`令负类锚框和填充锚框不参与损失的计算。\n",
    "最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "883cf50e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:25.891762Z",
     "iopub.status.busy": "2022-12-07T17:38:25.891353Z",
     "iopub.status.idle": "2022-12-07T17:38:25.896686Z",
     "shell.execute_reply": "2022-12-07T17:38:25.895957Z"
    },
    "origin_pos": 56,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:28.239232Z",
     "start_time": "2023-08-21T16:30:28.234754Z"
    }
   },
   "outputs": [],
   "source": [
    "cls_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "bbox_loss = nn.L1Loss(reduction='none')\n",
    "\n",
    "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
    "    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n",
    "    cls = cls_loss(cls_preds.reshape(-1, num_classes),\n",
    "                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\n",
    "    bbox = bbox_loss(bbox_preds * bbox_masks,\n",
    "                     bbox_labels * bbox_masks).mean(dim=1)\n",
    "    return cls + bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff9334",
   "metadata": {
    "origin_pos": 58
   },
   "source": [
    "我们可以沿用准确率评价分类结果。\n",
    "由于偏移量使用了$L_1$范数损失，我们使用*平均绝对误差*来评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "63bb405a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:25.900057Z",
     "iopub.status.busy": "2022-12-07T17:38:25.899443Z",
     "iopub.status.idle": "2022-12-07T17:38:25.904023Z",
     "shell.execute_reply": "2022-12-07T17:38:25.903271Z"
    },
    "origin_pos": 60,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:28.239359Z",
     "start_time": "2023-08-21T16:30:28.237367Z"
    }
   },
   "outputs": [],
   "source": [
    "def cls_eval(cls_preds, cls_labels):\n",
    "    # 由于类别预测结果放在最后一维，argmax需要指定最后一维。\n",
    "    return float((cls_preds.argmax(dim=-1).type(\n",
    "        cls_labels.dtype) == cls_labels).sum())\n",
    "\n",
    "def bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n",
    "    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n",
    "    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\n",
    "\n",
    "    Defined in :numref:`sec_anchor`\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n",
    "    # box i and the ground-truth bounding box j\n",
    "    jaccard = d2l.box_iou(anchors, ground_truth)\n",
    "    # Initialize the tensor to hold the assigned ground-truth bounding box for\n",
    "    # each anchor\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n",
    "                                  device=device)\n",
    "    # Assign ground-truth bounding boxes according to the threshold\n",
    "    max_ious, indices = torch.max(jaccard, dim=1)\n",
    "    anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1)\n",
    "    box_j = indices[max_ious >= 0.5]\n",
    "    anchors_bbox_map[anc_i] = box_j\n",
    "    col_discard = torch.full((num_anchors,), -1)\n",
    "    row_discard = torch.full((num_gt_boxes,), -1)\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = torch.argmax(jaccard)  # Find the largest IoU\n",
    "        max_idx = torch.tensor(max_idx, dtype=torch.long)\n",
    "        box_idx = (max_idx % num_gt_boxes).long()\n",
    "        anc_idx = (max_idx / num_gt_boxes).long()\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[:, box_idx] = col_discard\n",
    "        jaccard[anc_idx, :] = row_discard\n",
    "    return anchors_bbox_map\n",
    "\n",
    "def multibox_target(anchors, labels):\n",
    "    \"\"\"Label anchor boxes using ground-truth bounding boxes.\n",
    "\n",
    "    Defined in :numref:`subsec_labeling-anchor-boxes`\"\"\"\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    device, num_anchors = anchors.device, anchors.shape[0]\n",
    "    # print(device)\n",
    "    for i in range(batch_size):\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(\n",
    "            label[:, 1:], anchors, device)\n",
    "        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n",
    "            1, 4)\n",
    "        # Initialize class labels and assigned bounding box coordinates with\n",
    "        # zeros\n",
    "        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n",
    "                                   device=device)\n",
    "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n",
    "                                  device=device)\n",
    "        # Label classes of anchor boxes using their assigned ground-truth\n",
    "        # bounding boxes. If an anchor box is not assigned any, we label its\n",
    "        # class as background (the value remains zero)\n",
    "        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n",
    "        bb_idx = anchors_bbox_map[indices_true]\n",
    "        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
    "        # Offset transformation\n",
    "        offset = d2l.offset_boxes(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    class_labels = torch.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T16:30:28.245364Z",
     "start_time": "2023-08-21T16:30:28.243233Z"
    }
   },
   "id": "85c4d918df782254"
  },
  {
   "cell_type": "markdown",
   "id": "20b502ea",
   "metadata": {
    "origin_pos": 62
   },
   "source": [
    "### [**训练模型**]\n",
    "\n",
    "在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框（`anchors`），并预测其类别（`cls_preds`）和偏移量（`bbox_preds`）。\n",
    "然后，我们根据标签信息`Y`为生成的锚框标记类别（`cls_labels`）和偏移量（`bbox_labels`）。\n",
    "最后，我们根据类别和偏移量的预测和标注值计算损失函数。为了代码简洁，这里没有评价测试数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "62b01d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:38:25.907293Z",
     "iopub.status.busy": "2022-12-07T17:38:25.906714Z",
     "iopub.status.idle": "2022-12-07T17:39:43.987460Z",
     "shell.execute_reply": "2022-12-07T17:39:43.986632Z"
    },
    "origin_pos": 64,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[203], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m X, Y \u001B[38;5;241m=\u001B[39m features\u001B[38;5;241m.\u001B[39mto(device), target\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# 生成多尺度的锚框，为每个锚框预测类别和偏移量\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m anchors, cls_preds, bbox_preds \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# 为每个锚框标注类别和偏移量\u001B[39;00m\n\u001B[1;32m     17\u001B[0m bbox_labels, bbox_masks, cls_labels \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mmultibox_target(anchors, Y)\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[196], line 18\u001B[0m, in \u001B[0;36mTinySSD.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     15\u001B[0m anchors, cls_preds, bbox_preds \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m, [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m, [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m# getattr(self,'blk_%d'%i)即访问self.blk_i\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m     X, anchors[i], cls_preds[i], bbox_preds[i] \u001B[38;5;241m=\u001B[39m \u001B[43mblk_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mblk_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msizes\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mratios\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcls_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbbox_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m anchors \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(anchors, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     22\u001B[0m cls_preds \u001B[38;5;241m=\u001B[39m concat_preds(cls_preds)\n",
      "Cell \u001B[0;32mIn[194], line 4\u001B[0m, in \u001B[0;36mblk_forward\u001B[0;34m(X, blk, size, ratio, cls_predictor, bbox_predictor)\u001B[0m\n\u001B[1;32m      2\u001B[0m Y \u001B[38;5;241m=\u001B[39m blk(X)\n\u001B[1;32m      3\u001B[0m anchors \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mmultibox_prior(Y, sizes\u001B[38;5;241m=\u001B[39msize, ratios\u001B[38;5;241m=\u001B[39mratio)\n\u001B[0;32m----> 4\u001B[0m cls_preds \u001B[38;5;241m=\u001B[39m \u001B[43mcls_predictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m bbox_preds \u001B[38;5;241m=\u001B[39m bbox_predictor(Y)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (Y, anchors, cls_preds, bbox_preds)\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/d2l-zh/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 350x250 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"247.282812pt\" height=\"182.290625pt\" viewBox=\"0 0 247.282812 182.290625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-08-22T00:33:28.991850</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 182.290625 \nL 247.282812 182.290625 \nL 247.282812 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 39.221875 145.8 \nL 234.521875 145.8 \nL 234.521875 7.2 \nL 39.221875 7.2 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"text_1\">\n      <!-- 5 -->\n      <g style=\"fill: #262626\" transform=\"translate(77.557196 159.957813) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-35\" d=\"M 266 1200 \nL 856 1250 \nQ 922 819 1161 601 \nQ 1400 384 1738 384 \nQ 2144 384 2425 690 \nQ 2706 997 2706 1503 \nQ 2706 1984 2436 2262 \nQ 2166 2541 1728 2541 \nQ 1456 2541 1237 2417 \nQ 1019 2294 894 2097 \nL 366 2166 \nL 809 4519 \nL 3088 4519 \nL 3088 3981 \nL 1259 3981 \nL 1013 2750 \nQ 1425 3038 1878 3038 \nQ 2478 3038 2890 2622 \nQ 3303 2206 3303 1553 \nQ 3303 931 2941 478 \nQ 2500 -78 1738 -78 \nQ 1113 -78 717 272 \nQ 322 622 266 1200 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g style=\"fill: #262626\" transform=\"translate(126.171464 159.957813) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-31\" d=\"M 2384 0 \nL 1822 0 \nL 1822 3584 \nQ 1619 3391 1289 3197 \nQ 959 3003 697 2906 \nL 697 3450 \nQ 1169 3672 1522 3987 \nQ 1875 4303 2022 4600 \nL 2384 4600 \nL 2384 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"ArialMT-30\" d=\"M 266 2259 \nQ 266 3072 433 3567 \nQ 600 4063 929 4331 \nQ 1259 4600 1759 4600 \nQ 2128 4600 2406 4451 \nQ 2684 4303 2865 4023 \nQ 3047 3744 3150 3342 \nQ 3253 2941 3253 2259 \nQ 3253 1453 3087 958 \nQ 2922 463 2592 192 \nQ 2263 -78 1759 -78 \nQ 1097 -78 719 397 \nQ 266 969 266 2259 \nz\nM 844 2259 \nQ 844 1131 1108 757 \nQ 1372 384 1759 384 \nQ 2147 384 2411 759 \nQ 2675 1134 2675 2259 \nQ 2675 3391 2411 3762 \nQ 2147 4134 1753 4134 \nQ 1366 4134 1134 3806 \nQ 844 3388 844 2259 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"text_3\">\n      <!-- 15 -->\n      <g style=\"fill: #262626\" transform=\"translate(177.566201 159.957813) scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"text_4\">\n      <!-- 20 -->\n      <g style=\"fill: #262626\" transform=\"translate(228.960938 159.957813) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-32\" d=\"M 3222 541 \nL 3222 0 \nL 194 0 \nQ 188 203 259 391 \nQ 375 700 629 1000 \nQ 884 1300 1366 1694 \nQ 2113 2306 2375 2664 \nQ 2638 3022 2638 3341 \nQ 2638 3675 2398 3904 \nQ 2159 4134 1775 4134 \nQ 1369 4134 1125 3890 \nQ 881 3647 878 3216 \nL 300 3275 \nQ 359 3922 746 4261 \nQ 1134 4600 1788 4600 \nQ 2447 4600 2831 4234 \nQ 3216 3869 3216 3328 \nQ 3216 3053 3103 2787 \nQ 2991 2522 2730 2228 \nQ 2469 1934 1863 1422 \nQ 1356 997 1212 845 \nQ 1069 694 975 541 \nL 3222 541 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-32\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- epoch -->\n     <g style=\"fill: #262626\" transform=\"translate(123.25 173.103125) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"ArialMT-65\" d=\"M 2694 1069 \nL 3275 997 \nQ 3138 488 2766 206 \nQ 2394 -75 1816 -75 \nQ 1088 -75 661 373 \nQ 234 822 234 1631 \nQ 234 2469 665 2931 \nQ 1097 3394 1784 3394 \nQ 2450 3394 2872 2941 \nQ 3294 2488 3294 1666 \nQ 3294 1616 3291 1516 \nL 816 1516 \nQ 847 969 1125 678 \nQ 1403 388 1819 388 \nQ 2128 388 2347 550 \nQ 2566 713 2694 1069 \nz\nM 847 1978 \nL 2700 1978 \nQ 2663 2397 2488 2606 \nQ 2219 2931 1791 2931 \nQ 1403 2931 1139 2672 \nQ 875 2413 847 1978 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-70\" d=\"M 422 -1272 \nL 422 3319 \nL 934 3319 \nL 934 2888 \nQ 1116 3141 1344 3267 \nQ 1572 3394 1897 3394 \nQ 2322 3394 2647 3175 \nQ 2972 2956 3137 2557 \nQ 3303 2159 3303 1684 \nQ 3303 1175 3120 767 \nQ 2938 359 2589 142 \nQ 2241 -75 1856 -75 \nQ 1575 -75 1351 44 \nQ 1128 163 984 344 \nL 984 -1272 \nL 422 -1272 \nz\nM 931 1641 \nQ 931 1000 1190 694 \nQ 1450 388 1819 388 \nQ 2194 388 2461 705 \nQ 2728 1022 2728 1688 \nQ 2728 2322 2467 2637 \nQ 2206 2953 1844 2953 \nQ 1484 2953 1207 2617 \nQ 931 2281 931 1641 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-6f\" d=\"M 213 1659 \nQ 213 2581 725 3025 \nQ 1153 3394 1769 3394 \nQ 2453 3394 2887 2945 \nQ 3322 2497 3322 1706 \nQ 3322 1066 3130 698 \nQ 2938 331 2570 128 \nQ 2203 -75 1769 -75 \nQ 1072 -75 642 372 \nQ 213 819 213 1659 \nz\nM 791 1659 \nQ 791 1022 1069 705 \nQ 1347 388 1769 388 \nQ 2188 388 2466 706 \nQ 2744 1025 2744 1678 \nQ 2744 2294 2464 2611 \nQ 2184 2928 1769 2928 \nQ 1347 2928 1069 2612 \nQ 791 2297 791 1659 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-63\" d=\"M 2588 1216 \nL 3141 1144 \nQ 3050 572 2676 248 \nQ 2303 -75 1759 -75 \nQ 1078 -75 664 370 \nQ 250 816 250 1647 \nQ 250 2184 428 2587 \nQ 606 2991 970 3192 \nQ 1334 3394 1763 3394 \nQ 2303 3394 2647 3120 \nQ 2991 2847 3088 2344 \nL 2541 2259 \nQ 2463 2594 2264 2762 \nQ 2066 2931 1784 2931 \nQ 1359 2931 1093 2626 \nQ 828 2322 828 1663 \nQ 828 994 1084 691 \nQ 1341 388 1753 388 \nQ 2084 388 2306 591 \nQ 2528 794 2588 1216 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-68\" d=\"M 422 0 \nL 422 4581 \nL 984 4581 \nL 984 2938 \nQ 1378 3394 1978 3394 \nQ 2347 3394 2619 3248 \nQ 2891 3103 3008 2847 \nQ 3125 2591 3125 2103 \nL 3125 0 \nL 2563 0 \nL 2563 2103 \nQ 2563 2525 2380 2717 \nQ 2197 2909 1863 2909 \nQ 1613 2909 1392 2779 \nQ 1172 2650 1078 2428 \nQ 984 2206 984 1816 \nL 984 0 \nL 422 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-65\"/>\n      <use xlink:href=\"#ArialMT-70\" x=\"55.615234\"/>\n      <use xlink:href=\"#ArialMT-6f\" x=\"111.230469\"/>\n      <use xlink:href=\"#ArialMT-63\" x=\"166.845703\"/>\n      <use xlink:href=\"#ArialMT-68\" x=\"216.845703\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"text_6\">\n      <!-- 0.005 -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 146.103833) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-2e\" d=\"M 581 0 \nL 581 641 \nL 1222 641 \nL 1222 0 \nL 581 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n       <use xlink:href=\"#ArialMT-35\" x=\"194.628906\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"text_7\">\n      <!-- 0.010 -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 107.897723) scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-31\" x=\"139.013672\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"194.628906\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"text_8\">\n      <!-- 0.015 -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 69.691612) scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-31\" x=\"139.013672\"/>\n       <use xlink:href=\"#ArialMT-35\" x=\"194.628906\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"text_9\">\n      <!-- 0.020 -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 31.485501) scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n       <use xlink:href=\"#ArialMT-32\" x=\"139.013672\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"194.628906\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_1\">\n    <path d=\"M 39.221875 13.5 \nL 49.500822 138.583606 \n\" clip-path=\"url(#pb7bd04e27c)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: round\"/>\n   </g>\n   <g id=\"line2d_2\">\n    <path d=\"M 39.221875 135.558103 \nL 49.500822 139.5 \n\" clip-path=\"url(#pb7bd04e27c)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 39.221875 145.8 \nL 39.221875 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 234.521875 145.8 \nL 234.521875 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 39.221875 145.8 \nL 234.521875 145.8 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 39.221875 7.2 \nL 234.521875 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 148.85 43.490625 \nL 227.521875 43.490625 \nQ 229.521875 43.490625 229.521875 41.490625 \nL 229.521875 14.2 \nQ 229.521875 12.2 227.521875 12.2 \nL 148.85 12.2 \nQ 146.85 12.2 146.85 14.2 \nL 146.85 41.490625 \nQ 146.85 43.490625 148.85 43.490625 \nz\n\" style=\"fill: #eaeaf2; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_3\">\n     <path d=\"M 150.85 19.857812 \nL 160.85 19.857812 \nL 170.85 19.857812 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: round\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- class error -->\n     <g style=\"fill: #262626\" transform=\"translate(178.85 23.357812) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"ArialMT-6c\" d=\"M 409 0 \nL 409 4581 \nL 972 4581 \nL 972 0 \nL 409 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-61\" d=\"M 2588 409 \nQ 2275 144 1986 34 \nQ 1697 -75 1366 -75 \nQ 819 -75 525 192 \nQ 231 459 231 875 \nQ 231 1119 342 1320 \nQ 453 1522 633 1644 \nQ 813 1766 1038 1828 \nQ 1203 1872 1538 1913 \nQ 2219 1994 2541 2106 \nQ 2544 2222 2544 2253 \nQ 2544 2597 2384 2738 \nQ 2169 2928 1744 2928 \nQ 1347 2928 1158 2789 \nQ 969 2650 878 2297 \nL 328 2372 \nQ 403 2725 575 2942 \nQ 747 3159 1072 3276 \nQ 1397 3394 1825 3394 \nQ 2250 3394 2515 3294 \nQ 2781 3194 2906 3042 \nQ 3031 2891 3081 2659 \nQ 3109 2516 3109 2141 \nL 3109 1391 \nQ 3109 606 3145 398 \nQ 3181 191 3288 0 \nL 2700 0 \nQ 2613 175 2588 409 \nz\nM 2541 1666 \nQ 2234 1541 1622 1453 \nQ 1275 1403 1131 1340 \nQ 988 1278 909 1158 \nQ 831 1038 831 891 \nQ 831 666 1001 516 \nQ 1172 366 1500 366 \nQ 1825 366 2078 508 \nQ 2331 650 2450 897 \nQ 2541 1088 2541 1459 \nL 2541 1666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-73\" d=\"M 197 991 \nL 753 1078 \nQ 800 744 1014 566 \nQ 1228 388 1613 388 \nQ 2000 388 2187 545 \nQ 2375 703 2375 916 \nQ 2375 1106 2209 1216 \nQ 2094 1291 1634 1406 \nQ 1016 1563 777 1677 \nQ 538 1791 414 1992 \nQ 291 2194 291 2438 \nQ 291 2659 392 2848 \nQ 494 3038 669 3163 \nQ 800 3259 1026 3326 \nQ 1253 3394 1513 3394 \nQ 1903 3394 2198 3281 \nQ 2494 3169 2634 2976 \nQ 2775 2784 2828 2463 \nL 2278 2388 \nQ 2241 2644 2061 2787 \nQ 1881 2931 1553 2931 \nQ 1166 2931 1000 2803 \nQ 834 2675 834 2503 \nQ 834 2394 903 2306 \nQ 972 2216 1119 2156 \nQ 1203 2125 1616 2013 \nQ 2213 1853 2448 1751 \nQ 2684 1650 2818 1456 \nQ 2953 1263 2953 975 \nQ 2953 694 2789 445 \nQ 2625 197 2315 61 \nQ 2006 -75 1616 -75 \nQ 969 -75 630 194 \nQ 291 463 197 991 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-72\" d=\"M 416 0 \nL 416 3319 \nL 922 3319 \nL 922 2816 \nQ 1116 3169 1280 3281 \nQ 1444 3394 1641 3394 \nQ 1925 3394 2219 3213 \nL 2025 2691 \nQ 1819 2813 1613 2813 \nQ 1428 2813 1281 2702 \nQ 1134 2591 1072 2394 \nQ 978 2094 978 1738 \nL 978 0 \nL 416 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-63\"/>\n      <use xlink:href=\"#ArialMT-6c\" x=\"50\"/>\n      <use xlink:href=\"#ArialMT-61\" x=\"72.216797\"/>\n      <use xlink:href=\"#ArialMT-73\" x=\"127.832031\"/>\n      <use xlink:href=\"#ArialMT-73\" x=\"177.832031\"/>\n      <use xlink:href=\"#ArialMT-20\" x=\"227.832031\"/>\n      <use xlink:href=\"#ArialMT-65\" x=\"255.615234\"/>\n      <use xlink:href=\"#ArialMT-72\" x=\"311.230469\"/>\n      <use xlink:href=\"#ArialMT-72\" x=\"344.53125\"/>\n      <use xlink:href=\"#ArialMT-6f\" x=\"377.832031\"/>\n      <use xlink:href=\"#ArialMT-72\" x=\"433.447266\"/>\n     </g>\n    </g>\n    <g id=\"line2d_4\">\n     <path d=\"M 150.85 34.003125 \nL 160.85 34.003125 \nL 170.85 34.003125 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- bbox mae -->\n     <g style=\"fill: #262626\" transform=\"translate(178.85 37.503125) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"ArialMT-62\" d=\"M 941 0 \nL 419 0 \nL 419 4581 \nL 981 4581 \nL 981 2947 \nQ 1338 3394 1891 3394 \nQ 2197 3394 2470 3270 \nQ 2744 3147 2920 2923 \nQ 3097 2700 3197 2384 \nQ 3297 2069 3297 1709 \nQ 3297 856 2875 390 \nQ 2453 -75 1863 -75 \nQ 1275 -75 941 416 \nL 941 0 \nz\nM 934 1684 \nQ 934 1088 1097 822 \nQ 1363 388 1816 388 \nQ 2184 388 2453 708 \nQ 2722 1028 2722 1663 \nQ 2722 2313 2464 2622 \nQ 2206 2931 1841 2931 \nQ 1472 2931 1203 2611 \nQ 934 2291 934 1684 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-78\" d=\"M 47 0 \nL 1259 1725 \nL 138 3319 \nL 841 3319 \nL 1350 2541 \nQ 1494 2319 1581 2169 \nQ 1719 2375 1834 2534 \nL 2394 3319 \nL 3066 3319 \nL 1919 1756 \nL 3153 0 \nL 2463 0 \nL 1781 1031 \nL 1600 1309 \nL 728 0 \nL 47 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-6d\" d=\"M 422 0 \nL 422 3319 \nL 925 3319 \nL 925 2853 \nQ 1081 3097 1340 3245 \nQ 1600 3394 1931 3394 \nQ 2300 3394 2536 3241 \nQ 2772 3088 2869 2813 \nQ 3263 3394 3894 3394 \nQ 4388 3394 4653 3120 \nQ 4919 2847 4919 2278 \nL 4919 0 \nL 4359 0 \nL 4359 2091 \nQ 4359 2428 4304 2576 \nQ 4250 2725 4106 2815 \nQ 3963 2906 3769 2906 \nQ 3419 2906 3187 2673 \nQ 2956 2441 2956 1928 \nL 2956 0 \nL 2394 0 \nL 2394 2156 \nQ 2394 2531 2256 2718 \nQ 2119 2906 1806 2906 \nQ 1569 2906 1367 2781 \nQ 1166 2656 1075 2415 \nQ 984 2175 984 1722 \nL 984 0 \nL 422 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-62\"/>\n      <use xlink:href=\"#ArialMT-62\" x=\"55.615234\"/>\n      <use xlink:href=\"#ArialMT-6f\" x=\"111.230469\"/>\n      <use xlink:href=\"#ArialMT-78\" x=\"166.845703\"/>\n      <use xlink:href=\"#ArialMT-20\" x=\"216.845703\"/>\n      <use xlink:href=\"#ArialMT-6d\" x=\"244.628906\"/>\n      <use xlink:href=\"#ArialMT-61\" x=\"327.929688\"/>\n      <use xlink:href=\"#ArialMT-65\" x=\"383.544922\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb7bd04e27c\">\n   <rect x=\"39.221875\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs, timer = 20, d2l.Timer()\n",
    "animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                        legend=['class error', 'bbox mae'])\n",
    "net = net.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练精确度的和，训练精确度的和中的示例数\n",
    "    # 绝对误差的和，绝对误差的和中的示例数\n",
    "    metric = d2l.Accumulator(4)\n",
    "    net.train()\n",
    "    for features, target in train_iter:\n",
    "        timer.start()\n",
    "        trainer.zero_grad()\n",
    "        X, Y = features.to(device), target.to(device)\n",
    "        # 生成多尺度的锚框，为每个锚框预测类别和偏移量\n",
    "        anchors, cls_preds, bbox_preds = net(X)\n",
    "        # 为每个锚框标注类别和偏移量\n",
    "        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)\n",
    "        # 根据类别和偏移量的预测和标注值计算损失函数\n",
    "        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
    "                      bbox_masks)\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),\n",
    "                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n",
    "                   bbox_labels.numel())\n",
    "    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]\n",
    "    animator.add(epoch + 1, (cls_err, bbox_mae))\n",
    "print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')\n",
    "print(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on '\n",
    "      f'{str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ac44e",
   "metadata": {
    "origin_pos": 66
   },
   "source": [
    "## [**预测目标**]\n",
    "\n",
    "在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d67cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:39:43.990635Z",
     "iopub.status.busy": "2022-12-07T17:39:43.990349Z",
     "iopub.status.idle": "2022-12-07T17:39:43.995741Z",
     "shell.execute_reply": "2022-12-07T17:39:43.994902Z"
    },
    "origin_pos": 68,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-08-21T16:33:29.007363Z",
     "start_time": "2023-08-21T16:33:29.007110Z"
    }
   },
   "outputs": [],
   "source": [
    "X = torchvision.io.read_image('../img/banana.jpg').unsqueeze(0).float()\n",
    "img = X.squeeze(0).permute(1, 2, 0).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091cd0e",
   "metadata": {
    "origin_pos": 70
   },
   "source": [
    "使用下面的`multibox_detection`函数，我们可以根据锚框及其预测偏移量得到预测边界框。然后，通过非极大值抑制来移除相似的预测边界框。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18472826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:39:43.998768Z",
     "iopub.status.busy": "2022-12-07T17:39:43.998462Z",
     "iopub.status.idle": "2022-12-07T17:39:44.599636Z",
     "shell.execute_reply": "2022-12-07T17:39:44.598825Z"
    },
    "origin_pos": 72,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "start_time": "2023-08-21T16:33:29.008727Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    net.eval()\n",
    "    # device = torch.device('cpu')\n",
    "    net.to(device)\n",
    "    anchors, cls_preds, bbox_preds = net(X.to(device))\n",
    "    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)\n",
    "    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\n",
    "    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n",
    "    return output[0, idx]\n",
    "\n",
    "output = predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dd31e",
   "metadata": {
    "origin_pos": 74
   },
   "source": [
    "最后，我们[**筛选所有置信度不低于0.9的边界框，做为最终输出**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def84610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:39:44.603221Z",
     "iopub.status.busy": "2022-12-07T17:39:44.602675Z",
     "iopub.status.idle": "2022-12-07T17:39:44.919118Z",
     "shell.execute_reply": "2022-12-07T17:39:44.918305Z"
    },
    "origin_pos": 76,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "start_time": "2023-08-21T16:33:29.009521Z"
    }
   },
   "outputs": [],
   "source": [
    "def display(img, output, threshold):\n",
    "    d2l.set_figsize((5, 5))\n",
    "    fig = d2l.plt.imshow(img)\n",
    "    for row in output:\n",
    "        score = float(row[1])\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        h, w = img.shape[0:2]\n",
    "        bbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]\n",
    "        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n",
    "\n",
    "display(img, output.cpu(), threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324dc27",
   "metadata": {
    "origin_pos": 78
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 单发多框检测是一种多尺度目标检测模型。基于基础网络块和各个多尺度特征块，单发多框检测生成不同数量和不同大小的锚框，并通过预测这些锚框的类别和偏移量检测不同大小的目标。\n",
    "* 在训练单发多框检测模型时，损失函数是根据锚框的类别和偏移量的预测及标注值计算得出的。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 能通过改进损失函数来改进单发多框检测吗？例如，将预测偏移量用到的$L_1$范数损失替换为平滑$L_1$范数损失。它在零点附近使用平方函数从而更加平滑，这是通过一个超参数$\\sigma$来控制平滑区域的：\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "    \\begin{cases}\n",
    "    (\\sigma x)^2/2,& \\text{if }|x| < 1/\\sigma^2\\\\\n",
    "    |x|-0.5/\\sigma^2,& \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "当$\\sigma$非常大时，这种损失类似于$L_1$范数损失。当它的值较小时，损失函数较平滑。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e0709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:39:44.926062Z",
     "iopub.status.busy": "2022-12-07T17:39:44.925778Z",
     "iopub.status.idle": "2022-12-07T17:39:45.100145Z",
     "shell.execute_reply": "2022-12-07T17:39:45.099389Z"
    },
    "origin_pos": 80,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "start_time": "2023-08-21T16:33:29.010170Z"
    }
   },
   "outputs": [],
   "source": [
    "def smooth_l1(data, scalar):\n",
    "    out = []\n",
    "    for i in data:\n",
    "        if abs(i) < 1 / (scalar ** 2):\n",
    "            out.append(((scalar * i) ** 2) / 2)\n",
    "        else:\n",
    "            out.append(abs(i) - 0.5 / (scalar ** 2))\n",
    "    return torch.tensor(out)\n",
    "\n",
    "sigmas = [10, 1, 0.5]\n",
    "lines = ['-', '--', '-.']\n",
    "x = torch.arange(-2, 2, 0.1)\n",
    "d2l.set_figsize()\n",
    "\n",
    "for l, s in zip(lines, sigmas):\n",
    "    y = smooth_l1(x, scalar=s)\n",
    "    d2l.plt.plot(x, y, l, label='sigma=%.1f' % s)\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1c5a6",
   "metadata": {
    "origin_pos": 82
   },
   "source": [
    "此外，在类别预测时，实验中使用了交叉熵损失：设真实类别$j$的预测概率是$p_j$，交叉熵损失为$-\\log p_j$。我们还可以使用焦点损失 :cite:`Lin.Goyal.Girshick.ea.2017`。给定超参数$\\gamma > 0$和$\\alpha > 0$，此损失的定义为：\n",
    "\n",
    "$$ - \\alpha (1-p_j)^{\\gamma} \\log p_j.$$\n",
    "\n",
    "可以看到，增大$\\gamma$可以有效地减少正类预测概率较大时（例如$p_j > 0.5$）的相对损失，因此训练可以更集中在那些错误分类的困难示例上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1704b6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:39:45.103106Z",
     "iopub.status.busy": "2022-12-07T17:39:45.102822Z",
     "iopub.status.idle": "2022-12-07T17:39:45.264871Z",
     "shell.execute_reply": "2022-12-07T17:39:45.264117Z"
    },
    "origin_pos": 84,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "start_time": "2023-08-21T16:33:29.010982Z"
    }
   },
   "outputs": [],
   "source": [
    "def focal_loss(gamma, x):\n",
    "    return -(1 - x) ** gamma * torch.log(x)\n",
    "\n",
    "x = torch.arange(0.01, 1, 0.01)\n",
    "for l, gamma in zip(lines, [0, 1, 5]):\n",
    "    y = d2l.plt.plot(x, focal_loss(gamma, x), l, label='gamma=%.1f' % gamma)\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8e07b",
   "metadata": {
    "origin_pos": 86
   },
   "source": [
    "2. 由于篇幅限制，我们在本节中省略了单发多框检测模型的一些实现细节。能否从以下几个方面进一步改进模型：\n",
    "    1. 当目标比图像小得多时，模型可以将输入图像调大；\n",
    "    1. 通常会存在大量的负锚框。为了使类别分布更加平衡，我们可以将负锚框的高和宽减半；\n",
    "    1. 在损失函数中，给类别损失和偏移损失设置不同比重的超参数；\n",
    "    1. 使用其他方法评估目标检测模型，例如单发多框检测论文 :cite:`Liu.Anguelov.Erhan.ea.2016`中的方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab8da5",
   "metadata": {
    "origin_pos": 88,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/3204)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
